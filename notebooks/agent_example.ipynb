{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce997e8-fe91-4807-bdeb-07c0048148c4",
   "metadata": {},
   "source": [
    "# Before starting\n",
    "\n",
    "Be sure to have install `langchain>=1.0` and `langchain-openai>=1.1.9` (and of course `ToPG`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0f62b-dd50-45dd-961a-2d428fa2ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from topg import Topg\n",
    "\n",
    "# import langchain agents utilities \n",
    "from langchain.agents import AgentState, create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import ToolRuntime, tool\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbea02-e472-40b4-aba4-c0fe80b6451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "# Because we are in a notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4b225-3dd8-4c44-b7c2-93ea1d5cfdd7",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "First, let's get some already pre-processed data form the hf repository. Here hyperpropositions (propositions + entities) have already been extracted. We are going to use the HotPotQA dataset for example, but you could try with any other dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b54fc8-c32a-4c8e-b70f-ee881e70d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"mdelmas/Topg-kb\"\n",
    "\n",
    "with open(hf_hub_download(repo_id=REPO_ID, filename=\"hotpotqa_hyperpropositions.json\", repo_type=\"dataset\")) as f_hyper:\n",
    "    hyperpropositions = json.load(f_hyper)\n",
    "\n",
    "with open(hf_hub_download(repo_id=REPO_ID, filename=\"hotpotqa_passages.json\", repo_type=\"dataset\")) as f_passages:\n",
    "    passages = json.load(f_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c1645-d1b4-4415-8fcb-1767d26a1087",
   "metadata": {},
   "source": [
    "Just to make it faster we are only going to load the first 100 passages. Of course, you can try with more if you want !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643d45c-49a7-4e49-8474-6d0ec1704f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_passages = [passages[i] for i in range(100)]\n",
    "selected_passages_ids = set([p[\"passage_id\"] for p in selected_passages])\n",
    "selected_hyperpropositions = [hp for hp in hyperpropositions if hp['metadata']['passage_id'] in selected_passages_ids]\n",
    "\n",
    "# save them\n",
    "with open(\"selected_passages.json\", \"w\") as f_p_out:\n",
    "    json.dump(selected_passages, f_p_out, indent=4)\n",
    "with open(\"selected_hyperpropositions.json\", \"w\") as f_p_out:\n",
    "    json.dump(selected_hyperpropositions, f_p_out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b40c93-6f2c-4f2c-a16d-c876e338a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF IT IS THE FIRST TIME\n",
    "# Initialize a new KB at the given path\n",
    "config_path = Topg.initialize(\n",
    "    base_path=\"test\",\n",
    "    collection_name=\"MyKB\"\n",
    ")\n",
    "# IF NOT DO NOT RUN (it will return an error - the directory already exists.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41063be-d894-4015-b5d9-0be6eed9d1d4",
   "metadata": {},
   "source": [
    "# Load the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121941b-35c1-4199-b94d-f54e2d67058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generated config / you could manually edit the config before loading\n",
    "with open(config_path, \"r\") as f:\n",
    "    system_config = yaml.safe_load(f)\n",
    "\n",
    "# Mount the system\n",
    "system = Topg(config=system_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802281c-9a66-4735-8428-a1d479643042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import passages\n",
    "system.load_passages_from_json(json_passages=\"selected_passages.json\")\n",
    "\n",
    "# Import hyperpropositions\n",
    "system.load_hyperpropositions_from_json(json_hyperpropositions=\"selected_hyperpropositions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4e1ad-8cf0-4ceb-a642-c93be0f98a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.store.load_graphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b68129-d75a-4dae-afcc-b0892dc01956",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Let's try to just run one !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba87472-9f9d-497d-ab15-cbdf7b716bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a test !\n",
    "answer, memory = system.query(\n",
    "    question=\"The Distribution of Industry Act was passed by a man who was prime minister when?\",\n",
    "    mode=\"local\"\n",
    ")\n",
    "print(answer)\n",
    "print(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d3df8-24b6-4938-90f2-b269138c2340",
   "metadata": {},
   "source": [
    "`ToPG` does not have a routing mecanism by default, to decide which mode should be call (naive, local or global). In order to mimic one, we can assigned each ToPG mode a tool (`@tool`) and equip an agent with it.\n",
    "First, let's define the 3 tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466b57e-2ec9-4fcf-8ed8-501501f6dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\n",
    "    description=\"Use for simple factual, single-hop questions requiring direct factual retrieval. Produces short, fact-based answers.\"\n",
    ")\n",
    "def naive_qa(query: str) -> str:\n",
    "    answer, memory = system.query(\n",
    "        question=query,\n",
    "        mode=\"naive\"\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "@tool(\n",
    "    description=\"Use for complex questions requiring multi-hop retrieval. Produces short, fact-based answers.\"\n",
    ")\n",
    "def local_qa(query: str) -> str:\n",
    "    answer, memory = system.query(\n",
    "        question=query,\n",
    "        mode=\"local\"\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "@tool(\n",
    "    description=\"Use this tool for broad, analytical, or conceptual questions that require synthesis, explanation, or multi-factor reasoning. Produces long-form, comprehensive answers. Not for factual lookup.\"\n",
    ")\n",
    "def global_qa(query: str) -> str:\n",
    "    answer, memory = system.query(\n",
    "        question=query,\n",
    "        mode=\"global\"\n",
    "    )\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d9f3b-890d-4894-a916-45d81378ae39",
   "metadata": {},
   "source": [
    "Now, let's define a Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ff4ea-8ed8-41fd-a4b1-1e0b6920293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa61de8e-d1a9-4d40-a85b-6b5b89ab9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = (\n",
    "    \"You are a retrieval agent specialised in Question Answering.\"\n",
    "    \"You have access to three tools that can help you answer potential related question from the user: 'naive_qa', 'local_qa' and 'global_qa'.\"\n",
    "    \"The 'naive_qa' tool is for specific factual questions.\"\n",
    "    \"The 'local_qa' is used for more complex, multi-hop questions.\"\n",
    "    \"The 'global_qa' is used for abstract/conceptual questions when a long-form answer (mini report) is expected.\"\n",
    "    \"Otherwise, keep a normal flow of conversation with the user as a polite assistant.\"\n",
    ")\n",
    "\n",
    "class AgentState(AgentState):\n",
    "    user_id: str\n",
    "\n",
    "# A simple agent with the 3 tools\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[naive_qa, local_qa, global_qa], \n",
    "    system_prompt=system_prompt,\n",
    "    state_schema=AgentState,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13187eb-e410-4bdd-8801-145468192c8c",
   "metadata": {},
   "source": [
    "Let's try some query messages. For instance:\n",
    "- Tell me everything about the history of Loch Leven Castle (should call `global_qa`)\n",
    "- The Distribution of Industry Act was passed by a man who was prime minister when? (should call `naive_qa` or `local_qa`)\n",
    "- In which county is the town in which Raymond Robertsen was born ? (should call `naive_qa` or `local_qa`)\n",
    "\n",
    "\n",
    "These should trigger different tools !\n",
    "\n",
    "Below we are going to display the different messages and see the sequence (HumanMessage, ToolCall, ToolResponse, AssistantResponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa22d4-4fac-4efa-b709-f7fc1d1a3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me everything about the history of Loch Leven Castle\"}]\n",
    "})\n",
    "# then print\n",
    "for msg in result['messages']:\n",
    "    if isinstance(msg, HumanMessage):\n",
    "        print(f\"\\n\\n> Human message: {msg.content}\")\n",
    "    elif isinstance(msg, AIMessage):\n",
    "        if not msg.content:\n",
    "            # This is a tool message:\n",
    "            for tool_call in msg.tool_calls:\n",
    "                tool_name = tool_call.get(\"name\", \"unknown\")\n",
    "                tool_args = tool_call.get(\"args\", {})\n",
    "            \n",
    "                args_str = \"; \".join(\n",
    "                    [f\"{k}: {v}\" for k, v in tool_args.items()]\n",
    "                )\n",
    "            \n",
    "                print(f\"\\n\\n<< Calling Tool: {tool_name} with arguments: {args_str} >>\")\n",
    "        else:\n",
    "            print(f\"\\n\\n> Assistant Message: {msg.content}\")\n",
    "    elif isinstance(msg, ToolMessage):\n",
    "        tool_name = msg.name\n",
    "        tool_response = msg.content\n",
    "        print(f\"\\n\\n<< Tool {tool_name} response: {tool_response} >>\")\n",
    "    else:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
